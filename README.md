# Model-Interpretation

The most critical part of the Machine Learning model life cycle is to interpret models after training.For any machine learning project, it is really important for tweaking to find out why the model makes predictions the way it does. So, Interpreting models and the importance of each predictor should become second nature. LIME will make this easy to interpret the machine learning model.

# Lime

LIME stands for Local Interpretable Model-agnostic Explanations. It helps in understanding Tabular Models, Image, and Text Classifiers. This gives a brief explanation of what each predictor is doing in the prediction and lists out what features are contributing positively or negatively.

To read more about LIME, Please go to my blog - https://thinkdatascience.github.io/posts/LIME/
